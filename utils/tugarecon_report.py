# --------------------------------------------------------------------------------------------------
# TugaRecon â€“ Intelligence Report Generator (Corrected)
# Author: Skynet0x01 2020-2026
# License: GNU GPLv3
# --------------------------------------------------------------------------------------------------

from pathlib import Path
import subprocess
from datetime import datetime
import json
import shutil

# -------------------------------------------------------
def load_json(path):
    try:
        with open(path) as f:
            return json.load(f)
    except Exception as e:
        print(f"[!] Failed to load JSON {path}: {e}")
        return None

# -------------------------------------------------------
def sanitize_unicode(text: str) -> str:
    replacements = {
        "ðŸŸ¢": "[ALIVE]",
        "ðŸ”´": "[DEAD]",
        "ðŸŸ¡": "[MEDIUM]",
        "ðŸ§ ": "[INTEL]",
        "â€¢": "-",
    }
    for k, v in replacements.items():
        text = text.replace(k, v)
    return text

# -------------------------------------------------------
def generate_report(base_dir: Path, generate_pdf=False):
    """
    Generate Markdown & PDF intelligence report.
    base_dir = .../results/domain/date/report
    """

    report = []
    now = datetime.utcnow().isoformat()
    scan_root = base_dir.parent
    target = scan_root.parent.name

    base_dir.mkdir(parents=True, exist_ok=True)

    # -------------------------------------------------------
    # Header
    report.append("# TugaRecon Intelligence Report\n")
    report.append("**Generated by TugaRecon â€“ Advanced Reconnaissance & Intelligence Framework**  ")
    report.append("Author: skynet0x01  ")
    report.append("License: GNU GPLv3  ")
    report.append(f"Report generated on: {now} UTC  ")
    report.append(f"\nTarget: `{target}`\n")
    report.append("---\n")

    # -------------------------------------------------------
    # Priority Targets
    prio = scan_root / "attack_surface" / "priority_targets.txt"
    report.append("## Priority Targets")
    if prio.exists() and prio.read_text().strip():
        report.append("```\n" + prio.read_text().strip() + "\n```")
    else:
        report.append("_No priority targets detected._")

    # -------------------------------------------------------
    # Attack Surface Summary
    summary = scan_root / "attack_surface" / "attack_surface_summary.txt"
    report.append("\n## Attack Surface Summary")
    if summary.exists() and summary.read_text().strip():
        report.append("```\n" + summary.read_text().strip() + "\n```")
    else:
        report.append("_No summary available._")

    # -------------------------------------------------------
    # Semantic Intelligence
    semantic_path = scan_root / "semantic_results.json"
    semantic = load_json(semantic_path)

    report.append("\n## High Impact Semantic Targets")
    if semantic:
        count = 0
        for entry in semantic:
            # Compatibilidade: impact_score ou impact
            impact = entry.get("impact_score", entry.get("impact", 0))
            if impact < 1:
                continue

            subdomain = entry.get("subdomain", "unknown")
            url = entry.get("url", "N/A")
            status = entry.get("status", "N/A")
            scheme = entry.get("scheme", "N/A")
            tags = ", ".join(entry.get("tags", [])) if entry.get("tags") else "None"
            priority = entry.get("priority", entry.get("_priority", "LOW"))

            report.append(
                f"- **{subdomain}** (impact={impact}, priority={priority})\n"
                f"  - URL: {url}\n"
                f"  - Status: {status} | Scheme: {scheme}\n"
                f"  - Tags: {tags}\n"
            )
            count += 1
        if count == 0:
            report.append("_No high impact semantic targets detected._")
    else:
        report.append("_No semantic data available._")

    # -------------------------------------------------------
    # Temporal Intelligence
    diff = load_json(scan_root / "scan_diff.json")
    report.append("\n## Temporal Changes")
    if diff:
        for k, section in [("new", "Newly Discovered Assets"),
                           ("escalated", "Escalated Risk Assets"),
                           ("disappeared", "Disappeared Assets")]:
            if diff.get(k):
                report.append(f"\n### {section}")
                for item in diff[k]:
                    sub = item.get("subdomain", "unknown")
                    impact = item.get("impact_score", item.get("impact", 0))
                    priority = item.get("priority", item.get("_priority", "LOW"))
                    report.append(f"- {sub} â†’ impact_score: {impact}, priority: {priority}")
    else:
        report.append("_No temporal data available._")

    # -------------------------------------------------------
    # Probe Statistics
    web = scan_root / "probe" / "web_hosts.txt"
    dead = scan_root / "probe" / "dead_hosts.txt"
    report.append("\n## Probe Statistics")
    if web.exists():
        alive_count = len(web.read_text().splitlines())
        report.append(f"- Alive hosts: {alive_count}")
    if dead.exists():
        dead_count = len(dead.read_text().splitlines())
        report.append(f"- Dead hosts: {dead_count}")

    # -------------------------------------------------------
    # Save Markdown
    md_path = base_dir / "report.md"
    md_path.write_text(sanitize_unicode("\n".join(report)))

    # -------------------------------------------------------
    # Generate PDF
    pdf_path = None
    if generate_pdf:
        pdf_path = base_dir / "report.pdf"

        if not shutil.which("pandoc"):
            print("[!] Pandoc not found. Install pandoc to generate PDF reports.")
            return md_path, None

        try:
            result = subprocess.run(
                ["pandoc", str(md_path), "-o", str(pdf_path), "--pdf-engine=xelatex"],
                capture_output=True, text=True
            )
            if result.returncode != 0:
                print("[!] PDF generation failed:")
                print(result.stderr)
                pdf_path = None
            else:
                print(f"[âœ…] PDF generated successfully: {pdf_path}")
        except Exception as e:
            print(f"[!] PDF generation failed: {e}")
            pdf_path = None

    return md_path, pdf_path



# # --------------------------------------------------------------------------------------------------
# # TugaRecon â€“ report
# # Author: Skynet0x01 2020-2026
# # GitHub: https://github.com/skynet0x01/tugarecon
# # License: GNU GPLv3
# # Patent Restriction Notice:
# # No patents may be claimed or enforced on this software or any derivative.
# # Any patent claims will result in automatic termination of license rights under the GNU GPLv3.
# # --------------------------------------------------------------------------------------------------
# from pathlib import Path
# import subprocess
# from datetime import datetime
# import json
# import shutil
#
# def load_json(path):
#     try:
#         with open(path) as f:
#             return json.load(f)
#     except Exception as e:
#         print(f"[!] Failed to load JSON {path}: {e}")
#         return None
#
# def sanitize_unicode(text: str) -> str:
#     """Replace emojis/unicode symbols for PDF-friendly text."""
#     replacements = {
#         "ðŸŸ¢": "[ALIVE]",
#         "ðŸ”´": "[DEAD]",
#         "ðŸŸ¡": "[MEDIUM]",
#         "ðŸ§ ": "[INTEL]",
#         "â€¢": "-",  # bullet point
#     }
#     for k, v in replacements.items():
#         text = text.replace(k, v)
#     return text
#
# def generate_report(base_dir: Path, generate_pdf=False):
#     report = []
#     now = datetime.utcnow().isoformat()
#     target = base_dir.parent.name
#
#     # Criar a pasta caso nÃ£o exista
#     base_dir.mkdir(parents=True, exist_ok=True)
#
#     # -------------------------
#     # CabeÃ§alho profissional
#     # -------------------------
#     report.append("# TugaRecon Intelligence Report\n")
#     report.append("**Generated by TugaRecon â€“ Advanced Reconnaissance & Intelligence Framework**  ")
#     report.append("Author: skynet0x01  ")
#     report.append("License: GNU GPLv3  ")
#     report.append(f"Report generated on: {now} UTC  ")
#     report.append(f"\nTarget: `{target}`\n")
#     report.append("---\n")
#
#     # -------------------------
#     # Priority targets
#     # -------------------------
#     prio = base_dir / "attack_surface" / "priority_targets.txt"
#     report.append("## Priority Targets")
#     if prio.exists() and prio.read_text().strip():
#         report.append("```\n" + prio.read_text() + "\n```")
#     else:
#         report.append("_No priority targets detected._")
#
#     # -------------------------
#     # Attack surface summary
#     # -------------------------
#     summary = base_dir / "attack_surface" / "attack_surface_summary.txt"
#     report.append("\n## Attack Surface Summary")
#     if summary.exists() and summary.read_text().strip():
#         report.append(summary.read_text())
#     else:
#         report.append("_No summary available._")
#
#     # -------------------------
#     # Semantic Targets Overview
#     # -------------------------
#     semantic_path = base_dir.parent / "semantic_results.json"
#     if not semantic_path.exists():
#         print(f"[!] Semantic results not found at {semantic_path}")
#     semantic = load_json(semantic_path)
#
#     report.append("\n## Semantic Targets Overview")
#     if semantic:
#         # ordenar por impacto decrescente
#         semantic_sorted = sorted(semantic, key=lambda x: x.get("impact_score", 0), reverse=True)
#
#         for entry in semantic_sorted:
#             impact = entry.get("impact_score", 0)
#             subdomain = entry.get("subdomain", "unknown")
#             url = entry.get("url", "N/A")
#             status = entry.get("status", "N/A")
#             scheme = entry.get("scheme", "N/A")
#             tags = ", ".join(entry.get("tags", [])) if entry.get("tags") else "None"
#
#             # destacar impacto alto
#             if impact >= 70:
#                 impact_label = f"[HIGH] impact={impact}"
#             elif impact >= 30:
#                 impact_label = f"[MEDIUM] impact={impact}"
#             else:
#                 impact_label = f"[LOW] impact={impact}"
#
#             report.append(
#                 f"- **{subdomain}** ({impact_label})\n"
#                 f"  - URL: {url}\n"
#                 f"  - Status: {status} | Scheme: {scheme}\n"
#                 f"  - Tags: {tags}\n"
#             )
#     else:
#         report.append("_No semantic data available._")
#
#     # -------------------------
#     # Temporal changes
#     # -------------------------
#     diff = load_json(base_dir.parent / "scan_diff.json")
#     report.append("\n## Temporal Changes")
#     if diff:
#         for k, section in [("new", "Newly Discovered Assets"),
#                            ("escalated", "Escalated Risk Assets"),
#                            ("disappeared", "Disappeared Assets")]:
#             if diff.get(k):
#                 report.append(f"\n### {section}")
#                 for item in diff[k]:
#                     sub = item.get("subdomain", "unknown")
#                     impact = item.get("impact_score", item.get("impact", 0))
#                     priority = item.get("priority", item.get("_priority", "LOW"))
#                     report.append(f"- {sub} â†’ impact_score: {impact}, priority: {priority}")
#     else:
#         report.append("_No temporal data available._")
#
#     # -------------------------
#     # Probe statistics
#     # -------------------------
#     web = base_dir / "probe" / "web_hosts.txt"
#     dead = base_dir / "probe" / "dead_hosts.txt"
#     report.append("\n## Probe Statistics")
#     if web.exists():
#         alive_count = len(web.read_text().splitlines())
#         report.append(f"- Alive hosts: {alive_count}")
#     if dead.exists():
#         dead_count = len(dead.read_text().splitlines())
#         report.append(f"- Dead hosts: {dead_count}")
#
#     # -------------------------
#     # Salvar Markdown
#     # -------------------------
#     md_path = base_dir / "report.md"
#     md_path.write_text(sanitize_unicode("\n".join(report)))
#
#     # -------------------------
#     # Gerar PDF
#     # -------------------------
#     pdf_path = None
#     if generate_pdf:
#         pdf_path = base_dir / "report.pdf"
#
#         # verificar se pandoc existe
#         if not shutil.which("pandoc"):
#             print("[!] Pandoc not found. Install pandoc to generate PDF reports.")
#             return md_path, None
#
#         pdf_path.parent.mkdir(parents=True, exist_ok=True)
#         try:
#             result = subprocess.run(
#                 ["pandoc", str(md_path), "-o", str(pdf_path), "--pdf-engine=xelatex"],
#                 capture_output=True, text=True
#             )
#             if result.returncode != 0:
#                 print("[!] PDF generation failed:")
#                 print(result.stderr)
#                 pdf_path = None
#             else:
#                 print(f"[âœ…] PDF generated successfully: {pdf_path}")
#         except Exception as e:
#             print(f"[!] PDF generation failed: {e}")
#             pdf_path = None
#
#     return md_path, pdf_path


# # --------------------------------------------------------------------------------------------------
# # TugaRecon â€“ report
# # Author: Skynet0x01 2020-2026
# # GitHub: https://github.com/skynet0x01/tugarecon
# # License: GNU GPLv3
# # Patent Restriction Notice:
# # No patents may be claimed or enforced on this software or any derivative.
# # Any patent claims will result in automatic termination of license rights under the GNU GPLv3.
# # --------------------------------------------------------------------------------------------------
# from pathlib import Path
# import subprocess
# from datetime import datetime
# import json
# import shutil
# from collections import defaultdict
#
# def load_json(path):
#     try:
#         with open(path) as f:
#             return json.load(f)
#     except Exception:
#         return None
#
# def sanitize_unicode(text: str) -> str:
#     """Replace emojis/unicode symbols for PDF-friendly text."""
#     replacements = {
#         "ðŸŸ¢": "[ALIVE]",
#         "ðŸ”´": "[DEAD]",
#         "ðŸŸ¡": "[MEDIUM]",
#         "ðŸ§ ": "[INTEL]",
#         "â€¢": "-",  # bullet point
#     }
#     for k, v in replacements.items():
#         text = text.replace(k, v)
#     return text
#
# def generate_report(report_dir: Path, generate_pdf=False):
#     report = []
#     now = datetime.utcnow().isoformat()
#     day_dir = report_dir.parent
#     target = day_dir.name
#
#     report_dir.mkdir(parents=True, exist_ok=True)
#
#     # -------------------------
#     # CabeÃ§alho
#     # -------------------------
#     report.append("# TugaRecon Intelligence Report\n")
#     report.append("**Generated by TugaRecon â€“ Advanced Reconnaissance & Intelligence Framework**  ")
#     report.append("Author: skynet0x01  ")
#     report.append("License: GNU GPLv3  ")
#     report.append(f"Report generated on: {now} UTC  ")
#     report.append(f"\nTarget: `{target}`\n")
#     report.append("---\n")
#
#     # -------------------------
#     # Priority targets
#     # -------------------------
#     prio = day_dir / "attack_surface" / "priority_targets.txt"
#     report.append("## Priority Targets")
#     if prio.exists() and prio.read_text().strip():
#         report.append("```\n" + prio.read_text() + "\n```")
#     else:
#         report.append("_No priority targets detected._")
#
#     # -------------------------
#     # Attack surface summary
#     # -------------------------
#     summary = day_dir / "attack_surface" / "attack_surface_summary.txt"
#     report.append("\n## Attack Surface Summary")
#     if summary.exists() and summary.read_text().strip():
#         report.append(summary.read_text())
#     else:
#         report.append("_No summary available._")
#
#     # -------------------------
#     # Semantic results
#     # -------------------------
#     semantic_path = day_dir / "semantic_results.json"
#     semantic = load_json(semantic_path)
#
#     # -------------------------
#     # Top 10 Semantic Targets by Impact (ativos)
#     # -------------------------
#     report.append("\n## Top 10 Active Semantic Targets by Impact")
#     if semantic:
#         # filtrar apenas hosts vivos (status 2xx ou 3xx)
#         active_hosts = [s for s in semantic if str(s.get("status", 0)).startswith(("2","3"))]
#         top10 = sorted(active_hosts, key=lambda x: x.get("impact_score",0), reverse=True)[:10]
#         if top10:
#             for entry in top10:
#                 subdomain = entry.get("subdomain", "unknown")
#                 impact = entry.get("impact_score", 0)
#                 status = entry.get("status", "N/A")
#                 scheme = entry.get("scheme", "N/A")
#                 tags = ", ".join(entry.get("tags", [])) if entry.get("tags") else "None"
#                 url = entry.get("url", "N/A")
#                 report.append(f"- **{subdomain}** (impact={impact}, status={status}, scheme={scheme})")
#                 report.append(f"  - URL: {url}")
#                 report.append(f"  - Tags: {tags}")
#         else:
#             report.append("_No active semantic targets detected._")
#     else:
#         report.append("_No semantic data available._")
#
#     # -------------------------
#     # Grouped by status (ativos)
#     # -------------------------
#     report.append("\n## Active Hosts Grouped by HTTP Status")
#     if semantic:
#         grouped = defaultdict(list)
#         for entry in semantic:
#             status = entry.get("status", "N/A")
#             if str(status).startswith(("2","3")):  # apenas ativos
#                 grouped[status].append(entry)
#         if grouped:
#             for status in sorted(grouped.keys()):
#                 report.append(f"\n### Status {status}")
#                 for e in grouped[status]:
#                     sub = e.get("subdomain","unknown")
#                     url = e.get("url","N/A")
#                     impact = e.get("impact_score",0)
#                     tags = ", ".join(e.get("tags",[])) if e.get("tags") else "None"
#                     report.append(f"- **{sub}** â†’ impact={impact}, URL: {url}, tags: {tags}")
#         else:
#             report.append("_No active hosts found._")
#     else:
#         report.append("_No semantic data available._")
#
#     # -------------------------
#     # Temporal changes
#     # -------------------------
#     diff_path = day_dir / "scan_diff.json"
#     diff = load_json(diff_path)
#     report.append("\n## Temporal Changes")
#     if diff:
#         for k, section in [("new", "Newly Discovered Assets"),
#                            ("escalated", "Escalated Risk Assets"),
#                            ("disappeared", "Disappeared Assets")]:
#             if diff.get(k):
#                 report.append(f"\n### {section}")
#                 for item in diff[k]:
#                     sub = item.get("subdomain", "unknown")
#                     impact = item.get("impact_score", item.get("impact", 0))
#                     priority = item.get("priority", item.get("_priority","LOW"))
#                     report.append(f"- {sub} â†’ impact_score: {impact}, priority: {priority}")
#     else:
#         report.append("_No temporal data available._")
#
#     # -------------------------
#     # Probe statistics
#     # -------------------------
#     web = day_dir / "probe" / "web_hosts.txt"
#     dead = day_dir / "probe" / "dead_hosts.txt"
#     report.append("\n## Probe Statistics")
#     if web.exists():
#         report.append(f"- Alive hosts: {len(web.read_text().splitlines())}")
#     if dead.exists():
#         report.append(f"- Dead hosts: {len(dead.read_text().splitlines())}")
#
#     # -------------------------
#     # Salvar Markdown
#     # -------------------------
#     md_path = report_dir / "report.md"
#     md_path.write_text(sanitize_unicode("\n".join(report)))
#
#     # -------------------------
#     # Gerar PDF
#     # -------------------------
#     pdf_path = None
#     if generate_pdf:
#         pdf_path = report_dir / "report.pdf"
#
#         if not shutil.which("pandoc"):
#             print("[!] Pandoc not found. Install pandoc to generate PDF reports.")
#             return md_path, None
#
#         pdf_path.parent.mkdir(parents=True, exist_ok=True)
#         try:
#             result = subprocess.run(
#                 ["pandoc", str(md_path), "-o", str(pdf_path), "--pdf-engine=xelatex"],
#                 capture_output=True, text=True
#             )
#             if result.returncode != 0:
#                 print("[!] PDF generation failed:")
#                 print(result.stderr)
#                 pdf_path = None
#             else:
#                 print(f"[âœ…] PDF generated successfully: {pdf_path}")
#         except Exception as e:
#             print(f"[!] PDF generation failed: {e}")
#             pdf_path = None
#
#     return md_path, pdf_path


# # --------------------------------------------------------------------------------------------------
# # TugaRecon â€“ report
# # Author: Skynet0x01 2020-2026
# # GitHub: https://github.com/skynet0x01/tugarecon
# # License: GNU GPLv3
# # Patent Restriction Notice:
# # No patents may be claimed or enforced on this software or any derivative.
# # Any patent claims will result in automatic termination of license rights under the GNU GPLv3.
# # --------------------------------------------------------------------------------------------------
# from pathlib import Path
# import subprocess
# from datetime import datetime
# import json
# import shutil
#
# # -------------------------
# # Helpers
# # -------------------------
# def load_json(path):
#     try:
#         with open(path) as f:
#             return json.load(f)
#     except Exception as e:
#         print(f"[!] Failed to load JSON {path}: {e}")
#         return None
#
# def sanitize_unicode(text: str) -> str:
#     """Replace emojis/unicode symbols for PDF-friendly text."""
#     replacements = {
#         "ðŸŸ¢": "[ALIVE]",
#         "ðŸ”´": "[DEAD]",
#         "ðŸŸ¡": "[MEDIUM]",
#         "ðŸ§ ": "[INTEL]",
#         "â€¢": "-",  # bullet point
#     }
#     for k, v in replacements.items():
#         text = text.replace(k, v)
#     return text
#
# # -------------------------
# # Main Report Generator
# # -------------------------
# def generate_report(base_dir: Path, generate_pdf=False):
#     report = []
#     now = datetime.utcnow().isoformat()
#     target = base_dir.parent.name
#
#     # Criar a pasta caso nÃ£o exista
#     base_dir.mkdir(parents=True, exist_ok=True)
#
#     # -------------------------
#     # CabeÃ§alho profissional
#     # -------------------------
#     report.append("# TugaRecon Intelligence Report\n")
#     report.append("**Generated by TugaRecon â€“ Advanced Reconnaissance & Intelligence Framework**  ")
#     report.append("Author: skynet0x01  ")
#     report.append("License: GNU GPLv3  ")
#     report.append(f"Report generated on: {now} UTC  ")
#     report.append(f"\nTarget: `{target}`\n")
#     report.append("---\n")
#
#     # -------------------------
#     # Priority targets
#     # -------------------------
#     prio = base_dir / "attack_surface" / "priority_targets.txt"
#     report.append("## Priority Targets")
#     if prio.exists() and prio.read_text().strip():
#         report.append("```\n" + prio.read_text() + "\n```")
#     else:
#         report.append("_No priority targets detected._")
#
#     # -------------------------
#     # Attack surface summary
#     # -------------------------
#     summary = base_dir / "attack_surface" / "attack_surface_summary.txt"
#     report.append("\n## Attack Surface Summary")
#     if summary.exists() and summary.read_text().strip():
#         report.append(summary.read_text())
#     else:
#         report.append("_No summary available._")
#
#     # -------------------------
#     # High Impact Semantic Targets
#     # -------------------------
#     semantic_path = base_dir.parent / "semantic_results.json"
#     print(f"[*] Loading semantic JSON from: {semantic_path}")
#     semantic = load_json(semantic_path)
#
#     report.append("\n## High Impact Semantic Targets")
#     if semantic:
#         count = 0
#         for entry in semantic:
#             impact = entry.get("impact_score", 0)
#             if impact >= 50:  # mostrar todos os entries
#                 subdomain = entry.get("subdomain", "unknown")
#                 url = entry.get("url", "N/A")
#                 status = entry.get("status", "N/A")
#                 scheme = entry.get("scheme", "N/A")
#                 tags = ", ".join(entry.get("tags", [])) if entry.get("tags") else "None"
#
#                 report.append(
#                     f"- **{subdomain}** (impact={impact})\n"
#                     f"  - URL: {url}\n"
#                     f"  - Status: {status} | Scheme: {scheme}\n"
#                     f"  - Tags: {tags}\n"
#                 )
#                 count += 1
#         if count == 0:
#             report.append("_No high impact semantic targets detected._")
#     else:
#         report.append("_No semantic data available._")
#
#     # -------------------------
#     # Temporal changes
#     # -------------------------
#     diff = load_json(base_dir.parent / "scan_diff.json")
#     report.append("\n## Temporal Changes")
#     if diff:
#         for k, section in [("new", "Newly Discovered Assets"),
#                            ("escalated", "Escalated Risk Assets"),
#                            ("disappeared", "Disappeared Assets")]:
#             if diff.get(k):
#                 report.append(f"\n### {section}")
#                 for item in diff[k]:
#                     sub = item.get("subdomain", "unknown")
#                     impact = item.get("impact_score", item.get("impact", 0))
#                     priority = item.get("priority", item.get("_priority", "LOW"))
#                     report.append(f"- {sub} â†’ impact_score: {impact}, priority: {priority}")
#     else:
#         report.append("_No temporal data available._")
#
#     # -------------------------
#     # Probe statistics
#     # -------------------------
#     web = base_dir / "probe" / "web_hosts.txt"
#     dead = base_dir / "probe" / "dead_hosts.txt"
#     report.append("\n## Probe Statistics")
#     if web.exists():
#         alive_count = len(web.read_text().splitlines())
#         report.append(f"- Alive hosts: {alive_count}")
#     if dead.exists():
#         dead_count = len(dead.read_text().splitlines())
#         report.append(f"- Dead hosts: {len(dead.read_text().splitlines())}")
#
#     # -------------------------
#     # Salvar Markdown
#     # -------------------------
#     md_path = base_dir / "report.md"
#     md_path.write_text(sanitize_unicode("\n".join(report)))
#     print(f"[âœ…] Markdown report saved at: {md_path}")
#
#     # -------------------------
#     # Gerar PDF
#     # -------------------------
#     pdf_path = None
#     if generate_pdf:
#         pdf_path = base_dir / "report.pdf"
#
#         if not shutil.which("pandoc"):
#             print("[!] Pandoc not found. Install pandoc to generate PDF reports.")
#             return md_path, None
#
#         pdf_path.parent.mkdir(parents=True, exist_ok=True)
#         try:
#             result = subprocess.run(
#                 ["pandoc", str(md_path), "-o", str(pdf_path), "--pdf-engine=xelatex"],
#                 capture_output=True, text=True
#             )
#             if result.returncode != 0:
#                 print("[!] PDF generation failed:")
#                 print(result.stderr)
#                 pdf_path = None
#             else:
#                 print(f"[âœ…] PDF generated successfully: {pdf_path}")
#         except Exception as e:
#             print(f"[!] PDF generation failed: {e}")
#             pdf_path = None
#
#     return md_path, pdf_path
#
